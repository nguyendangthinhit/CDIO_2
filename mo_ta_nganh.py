import re
import json
import sys
import subprocess

# Thá»­ import, náº¿u chÆ°a cÃ³ thÃ¬ cÃ i
try:
    from docx import Document
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
    from docx import Document


def read_docx(path: str) -> str:
    """Äá»c toÃ n bá»™ ná»™i dung tá»« file .docx thÃ nh chuá»—i text."""
    doc = Document(path)
    return "\n".join(p.text for p in doc.paragraphs if p.text.strip())


def normalize_nganh_name(ten_nganh: str) -> str:
    """Chuáº©n hÃ³a tÃªn ngÃ nh: viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u má»—i tá»« chÃ­nh vÃ  xÃ³a tá»« 'NgÃ nh' thá»«a"""
    # XÃ³a tá»« "NgÃ nh" thá»«a náº¿u xuáº¥t hiá»‡n 2 láº§n
    # Pattern: tÃ¬m "NgÃ nh" + space + "NgÃ nh"
    ten_nganh = re.sub(r'\bNgÃ nh\s+NgÃ nh\b', 'NgÃ nh', ten_nganh, flags=re.IGNORECASE)
    
    # Danh sÃ¡ch tá»« khÃ´ng viáº¿t hoa
    lowercase_words = {'vÃ ', 'cá»§a', '-'}
    
    words = ten_nganh.split()
    normalized = []
    
    for i, word in enumerate(words):
        # Giá»¯ nguyÃªn dáº¥u gáº¡ch ngang
        if word == '-':
            normalized.append(word)
        # Tá»« Ä‘áº§u tiÃªn luÃ´n viáº¿t hoa
        elif i == 0:
            normalized.append(word.capitalize())
        # CÃ¡c tá»« Ä‘áº·c biá»‡t giá»¯ nguyÃªn chá»¯ thÆ°á»ng
        elif word.lower() in lowercase_words:
            normalized.append(word.lower())
        # CÃ¡c tá»« khÃ¡c viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u
        else:
            normalized.append(word.capitalize())
    
    return ' '.join(normalized)


def parse_structured_data(raw_text: str) -> dict:
    """Parse dá»¯ liá»‡u thÃ nh cáº¥u trÃºc, gá»™p theo mÃ£ ngÃ nh."""
    
    # TÃ¬m táº¥t cáº£ cÃ¡c pattern "NgÃ nh ... MÃ£ ngÃ nh: XXX"
    nganh_pattern = r'NgÃ nh\s+(.+?)\s*-\s*MÃ£ ngÃ nh:\s*(\d+)'
    
    nganh_matches = list(re.finditer(nganh_pattern, raw_text, re.IGNORECASE))
    
    if not nganh_matches:
        print("KhÃ´ng tÃ¬m tháº¥y ngÃ nh nÃ o!")
        return {}
    
    # Dictionary Ä‘á»ƒ lÆ°u táº¡m theo mÃ£ ngÃ nh
    temp_dict = {}
    
    # Xá»­ lÃ½ tá»«ng ngÃ nh
    for i, nganh_match in enumerate(nganh_matches):
        ten_nganh = nganh_match.group(1).strip()
        ma_nganh = nganh_match.group(2).strip()
        
        # Chuáº©n hÃ³a tÃªn ngÃ nh
        ten_nganh_normalized = normalize_nganh_name(ten_nganh)
        
        # TÃ¬m vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a ngÃ nh nÃ y
        start_pos = nganh_match.start()
        
        if i + 1 < len(nganh_matches):
            end_pos = nganh_matches[i + 1].start()
        else:
            end_pos = len(raw_text)
        
        # Láº¥y text cá»§a ngÃ nh nÃ y
        nganh_text = raw_text[start_pos:end_pos]
        
        # TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text nÃ y
        chuyen_nganh_list = find_chuyen_nganh_in_text(nganh_text, ten_nganh_normalized, ma_nganh)
        
        # Gá»™p theo mÃ£ ngÃ nh
        if chuyen_nganh_list:
            if ma_nganh not in temp_dict:
                temp_dict[ma_nganh] = {
                    'ten_nganh': ten_nganh_normalized,
                    'chuyen_nganh': []
                }
            
            temp_dict[ma_nganh]['chuyen_nganh'].extend(chuyen_nganh_list)
    
    # Chuyá»ƒn tá»« dict táº¡m sang dict cuá»‘i cÃ¹ng vá»›i key lÃ  tÃªn ngÃ nh
    result = {}
    for ma_nganh, data in temp_dict.items():
        ten_nganh = data['ten_nganh']
        # XÃ³a chá»¯ "NgÃ nh" á»Ÿ Ä‘áº§u náº¿u cÃ³ (vÃ¬ sáº½ thÃªm láº¡i sau)
        ten_nganh = re.sub(r'^NgÃ nh\s+', '', ten_nganh, flags=re.IGNORECASE)
        nganh_key = f"NgÃ nh {ten_nganh}"
        result[nganh_key] = data['chuyen_nganh']
    
    return result


def find_chuyen_nganh_in_text(nganh_text: str, ten_nganh: str, ma_nganh: str) -> list:
    """TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text cá»§a má»™t ngÃ nh."""
    
    # Patterns Ä‘á»ƒ tÃ¬m chuyÃªn ngÃ nh
    chuyen_nganh_patterns = [
        r'(?:TÃªn\s+)?ChuyÃªn ngÃ nh[:\s]+(.+?)\s*-\s*MÃ£ chuyÃªn ngÃ nh[:\s]*([A-Za-z0-9\s\(\)]+?)(?=\s*[:\n]|$)',
        r'ChuyÃªn ngÃ nh[:\s]+(.+?)\s*-\s*MÃ£[:\s]*([A-Za-z0-9\s\(\)]+?)(?=\s*[:\n]|$)'
    ]
    
    chuyen_nganh_matches = []
    
    # TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text
    for pattern in chuyen_nganh_patterns:
        matches = list(re.finditer(pattern, nganh_text, re.IGNORECASE))
        if matches:
            chuyen_nganh_matches = matches
            break
    
    if not chuyen_nganh_matches:
        print(f"  KhÃ´ng tÃ¬m tháº¥y chuyÃªn ngÃ nh nÃ o trong mÃ£ ngÃ nh {ma_nganh}")
        return []
    
    result = []
    
    # Xá»­ lÃ½ tá»«ng chuyÃªn ngÃ nh
    for i, match in enumerate(chuyen_nganh_matches):
        ten_chuyen_nganh = match.group(1).strip()
        ma_chuyen_nganh = match.group(2).strip()
        
        # TÃ¬m ná»™i dung cá»§a chuyÃªn ngÃ nh nÃ y
        content_start = match.end()
        
        # TÃ¬m Ä‘iá»ƒm káº¿t thÃºc
        if i + 1 < len(chuyen_nganh_matches):
            content_end = chuyen_nganh_matches[i + 1].start()
        else:
            next_nganh = re.search(r'NgÃ nh\s+.+?\s*-\s*MÃ£ ngÃ nh:', nganh_text[content_start:], re.IGNORECASE)
            if next_nganh:
                content_end = content_start + next_nganh.start()
            else:
                content_end = len(nganh_text)
        
        # Láº¥y ná»™i dung
        content_text = nganh_text[content_start:content_end].strip()
        
        # Parse cÃ¡c field content
        content_fields = parse_content_fields(content_text)
        
        # Táº¡o object chuyÃªn ngÃ nh - CHá»ˆ GIá»® Láº I ChuyÃªn NgÃ nh vÃ  cÃ¡c field content
        chuyen_nganh_obj = {
            "ChuyÃªn NgÃ nh": ten_chuyen_nganh,
            **content_fields
        }
        
        result.append(chuyen_nganh_obj)
        print(f"  âœ… MÃ£ {ma_nganh} - {ten_nganh} - CN: {ten_chuyen_nganh}")
    
    return result


def parse_content_fields(content_text: str) -> dict:
    """Parse cÃ¡c field: Giá»›i thiá»‡u, Má»¥c tiÃªu, ChÆ°Æ¡ng trÃ¬nh, CÆ¡ há»™i."""
    
    keyword_map = {
        "Giá»›i thiá»‡u:": "Giá»›i Thiá»‡u",
        "Má»¥c tiÃªu:": "Má»¥c TiÃªu",
        "ChÆ°Æ¡ng trÃ¬nh:": "ChÆ°Æ¡ng TrÃ¬nh", 
        "CÆ¡ há»™i:": "CÆ¡ Há»™i"
    }
    
    result = {}
    
    # Normalize content first
    content_text = re.sub(r'\n+', ' ', content_text)
    content_text = re.sub(r'\s+', ' ', content_text)
    
    # TÃ¬m positions cá»§a táº¥t cáº£ keywords
    keyword_positions = []
    for keyword in keyword_map.keys():
        pattern = rf'\b{re.escape(keyword)}[:\s]'
        for match in re.finditer(pattern, content_text, re.IGNORECASE):
            keyword_positions.append((match.start(), match.end(), keyword))
    
    # Sort theo position
    keyword_positions.sort()
    
    # Extract content cho tá»«ng keyword
    for i, (start, end, keyword) in enumerate(keyword_positions):
        if i + 1 < len(keyword_positions):
            next_start = keyword_positions[i + 1][0]
            section_text = content_text[end:next_start].strip()
        else:
            section_text = content_text[end:].strip()
        
        # Clean section text
        section_text = clean_section_content(section_text, keyword)
        
        if section_text and keyword in keyword_map:
            result[keyword_map[keyword]] = section_text
    
    return result


def clean_section_content(text: str, keyword: str) -> str:
    """Clean vÃ  format ná»™i dung cá»§a tá»«ng section."""
    if not text:
        return ""
    
    # Remove common prefixes
    prefixes_to_remove = [
        "Ä‘Ã o táº¡o:",
        "nghá» nghiá»‡p:",
    ]
    
    text_lower = text.lower()
    for prefix in prefixes_to_remove:
        if text_lower.startswith(prefix):
            text = text[len(prefix):].strip()
            break
    
    # Remove bullet points
    text = re.sub(r'^\s*â€¢\s*', '', text, flags=re.MULTILINE)
    
    # Truncate náº¿u cÃ³ dáº¥u hiá»‡u cá»§a section tiáº¿p theo
    skip_keywords = ["LiÃªn há»‡:", "Tuyá»ƒn sinh:", "Website:", "Email:", "Äiá»‡n thoáº¡i:"]
    for skip_kw in skip_keywords:
        pos = text.find(skip_kw)
        if pos > 0:
            text = text[:pos].strip()
            break
    
    # Truncate if text is too long
    if len(text) > 1000:
        for next_keyword in ["NgÃ nh", "ChuyÃªn ngÃ nh"]:
            pos = text.find(next_keyword)
            if pos > 100:
                text = text[:pos].strip()
                break
    
    return text.strip()


if __name__ == "__main__":
    files = [
        "TRÆ¯á»œNG CÃ”NG NGHá»†.docx",
        "KHOA ÄÃ€O Táº O QUá»C Táº¾.docx",
        "TRÆ¯á»œNG DU Lá»ŠCH.docx", 
        "TRÆ¯á»œNG KHOA Há»ŒC MÃY TÃNH.docx",
        "TRÆ¯á»œNG KINH Táº¾&KINH DOANH.docx",
        "TRÆ¯á»œNG NGÃ”N NGá»® & XÃƒ Há»˜I NHÃ‚N VÄ‚N.docx",
        "TRUNG TÃ‚M KT&CN VIá»†T-NHáº¬T.docx",
        "KHOA Y-DÆ¯á»¢C.docx"
    ]

    all_data = {}

    for file in files:
        try:
            print(f"\nğŸ” Processing {file}...")
            text = read_docx(file)
            if text.strip():
                result = parse_structured_data(text)
                file_key = file.replace(".docx", "")
                
                # Chá»‰ thÃªm vÃ o náº¿u cÃ³ dá»¯ liá»‡u
                if result:
                    all_data[file_key] = result
                    
                    # Äáº¿m sá»‘ chuyÃªn ngÃ nh
                    total_chuyen_nganh = sum(len(v) for v in result.values())
                    print(f"âœ… {file}: {len(result)} ngÃ nh, {total_chuyen_nganh} chuyÃªn ngÃ nh")
            else:
                print(f"âš ï¸  {file}: No content found")
        except Exception as e:
            print(f"âŒ Error processing {file}: {e}")

    # LÆ°u file JSON
    if all_data:
        with open("mo_ta_nganh.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u vÃ o mo_ta_nganh.json")
        
        # Thá»‘ng kÃª tá»•ng
        total_truong = len(all_data)
        total_nganh = sum(len(school) for school in all_data.values())
        total_chuyen_nganh = sum(
            sum(len(v) for v in school.values()) 
            for school in all_data.values()
        )
        print(f"ğŸ“Š Tá»•ng cá»™ng: {total_truong} trÆ°á»ng, {total_nganh} ngÃ nh, {total_chuyen_nganh} chuyÃªn ngÃ nh")
    else:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")
