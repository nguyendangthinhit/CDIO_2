import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import json
import re
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

class JobTrendMapper:
    def __init__(self):
        """
        Kh·ªüi t·∫°o AI Tuy·ªÉn Sinh Job Trend Mapper
        S·ª≠ d·ª•ng Sentence Transformer ƒë·ªÉ t√¨m similarity gi·ªØa ng√†nh ngh·ªÅ hot v√† ng√†nh h·ªçc DTU
        """
        print("ü§ñ Kh·ªüi t·∫°o AI Tuy·ªÉn Sinh Job Trend Mapper...")
        
        # Load model transformer (multilingual ƒë·ªÉ h·ªó tr·ª£ ti·∫øng Vi·ªát)
        try:
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("‚úÖ ƒê√£ load Sentence Transformer model")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load transformer model: {e}")
            print("üí° S·ª≠ d·ª•ng fallback similarity method")
            self.model = None
        
        # D·ªØ li·ªáu xu h∆∞·ªõng vi·ªác l√†m 2025
        self.job_trends = [
            {
                "nghe": "Technology-driven roles (IT, AI, blockchain, cybersecurity)",
                "nhu_cau": "Chuy·ªÉn ƒë·ªïi s·ªë, AI, blockchain v√† an ninh m·∫°ng ph√°t tri·ªÉn ‚Üí t·∫°o nhi·ªÅu vi·ªác l√†m m·ªõi, nhu c·∫ßu nh√¢n l·ª±c thi·∫øu h·ª•t ~30%",
                "luong": "‚Ç´37,750,000/th√°ng (AI/Blockchain Engineer, ~4 nƒÉm KN)",
                "tang_truong": "Vi·ªác l√†m AI tƒÉng ~74%/nƒÉm t·∫°i M·ªπ; Machine Learning Engineer tƒÉng ~344% t·ª´ 2015‚Äì2022",
                "nganh_goi_y": ["CNTT", "Khoa h·ªçc m√°y t√≠nh", "Tr√≠ tu·ªá nh√¢n t·∫°o", "An to√†n th√¥ng tin"]
            },
            {
                "nghe": "Green jobs ‚Äì NƒÉng l∆∞·ª£ng t√°i t·∫°o, ESG, m√¥i tr∆∞·ªùng",
                "nhu_cau": "Bi·∫øn ƒë·ªïi kh√≠ h·∫≠u, xu h∆∞·ªõng nƒÉng l∆∞·ª£ng s·∫°ch, doanh nghi·ªáp ch√∫ tr·ªçng ESG",
                "luong": "‚Ç´22,500,000/th√°ng (trung b√¨nh); ‚Ç´565M/nƒÉm (~‚Ç´47M/th√°ng)",
                "tang_truong": "70% c√¥ng ty to√†n c·∫ßu c√≥ k·∫ø ho·∫°ch tuy·ªÉn green workforce t·ª´ 2025",
                "nganh_goi_y": ["K·ªπ thu·∫≠t m√¥i tr∆∞·ªùng", "C√¥ng ngh·ªá k·ªπ thu·∫≠t nƒÉng l∆∞·ª£ng t√°i t·∫°o", "Qu·∫£n l√Ω t√†i nguy√™n m√¥i tr∆∞·ªùng"]
            },
            {
                "nghe": "Business/Sales, Marketing/Comms, Customer Service, HR",
                "nhu_cau": "Kinh t·∫ø tƒÉng tr∆∞·ªüng, nhu c·∫ßu kinh doanh ‚Äì d·ªãch v·ª• kh√°ch h√†ng cao",
                "luong": "‚Ç´8‚Äì20M/th√°ng (entry); >‚Ç´50M/th√°ng (qu·∫£n l√Ω)",
                "tang_truong": "TopCV: Business/Sales chi·∫øm 47.6% nhu c·∫ßu tuy·ªÉn, tƒÉng 8.3% so v·ªõi 2024",
                "nganh_goi_y": ["Marketing", "Qu·∫£n tr·ªã kinh doanh", "Truy·ªÅn th√¥ng ƒëa ph∆∞∆°ng ti·ªán", "Qu·∫£n tr·ªã nh√¢n l·ª±c"]
            },
            {
                "nghe": "IT/Software",
                "nhu_cau": "Ng√†nh c·ªët l√µi trong n·ªÅn kinh t·∫ø s·ªë, thi·∫øu h·ª•t k·ªπ s∆∞ ph·∫ßn m·ªÅm l·ªõn",
                "luong": "‚Ç´9.7‚Äì47.5M/th√°ng",
                "tang_truong": "Vi·ªát Nam thi·∫øu 150.000 l·∫≠p tr√¨nh vi√™n/nƒÉm",
                "nganh_goi_y": ["CNTT", "Khoa h·ªçc m√°y t√≠nh", "K·ªπ thu·∫≠t ph·∫ßn m·ªÅm"]
            },
            {
                "nghe": "E‚Äëcommerce, Logistics, Fintech, IT",
                "nhu_cau": "Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠, t√†i ch√≠nh s·ªë v√† logistics b√πng n·ªï",
                "luong": "‚Ç´12‚Äì25M/th√°ng",
                "tang_truong": "Nhu c·∫ßu nh√¢n l·ª±c tƒÉng m·∫°nh nh·ªù e‚Äëcommerce & fintech",
                "nganh_goi_y": ["Logistics & Qu·∫£n l√Ω chu·ªói cung ·ª©ng", "T√†i ch√≠nh ‚Äì Ng√¢n h√†ng", "Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", "C√¥ng ngh·ªá t√†i ch√≠nh"]
            },
            {
                "nghe": "Manufacturing, Processing",
                "nhu_cau": "68% c√¥ng ty s·∫£n xu·∫•t m·ªü r·ªông t·∫°i VN ‚Üí c·∫ßn nhi·ªÅu lao ƒë·ªông k·ªπ thu·∫≠t",
                "luong": "‚Ç´8‚Äì18M/th√°ng",
                "tang_truong": "68% c√¥ng ty s·∫£n xu·∫•t c√≥ k·∫ø ho·∫°ch tuy·ªÉn d·ª•ng th√™m",
                "nganh_goi_y": ["C√¥ng ngh·ªá k·ªπ thu·∫≠t c∆° kh√≠", "C√¥ng ngh·ªá ch·∫ø t·∫°o m√°y", "K·ªπ thu·∫≠t ƒëi·ªán", "T·ª± ƒë·ªông h√≥a"]
            },
            {
                "nghe": "Renewable Energy emergence",
                "nhu_cau": "TƒÉng tr∆∞·ªüng nƒÉng l∆∞·ª£ng t√°i t·∫°o, m·ªü r·ªông tuy·ªÉn d·ª•ng ng√†nh li√™n quan",
                "luong": "‚Ç´20‚Äì47M/th√°ng",
                "tang_truong": "Nhu c·∫ßu tƒÉng tr∆∞·ªüng cao trong lƒ©nh v·ª±c nƒÉng l∆∞·ª£ng t√°i t·∫°o",
                "nganh_goi_y": ["K·ªπ thu·∫≠t m√¥i tr∆∞·ªùng", "C√¥ng ngh·ªá k·ªπ thu·∫≠t nƒÉng l∆∞·ª£ng", "K·ªπ thu·∫≠t ƒëi·ªán"]
            },
            {
                "nghe": "AI integration in business/sales, IT, marketing, customer service, HR",
                "nhu_cau": "AI th√∫c ƒë·∫©y hi·ªáu qu·∫£ kinh doanh v√† d·ªãch v·ª• ‚Üí t·∫°o vi·ªác l√†m m·ªõi",
                "luong": "AI Engineers: ‚Ç´20‚Äì50M/th√°ng + th∆∞·ªüng",
                "tang_truong": "Tuy·ªÉn d·ª•ng AI Engineer tƒÉng 60% trong Q1/2025",
                "nganh_goi_y": ["CNTT", "Qu·∫£n tr·ªã kinh doanh", "Marketing", "Tr√≠ tu·ªá nh√¢n t·∫°o", "Khoa h·ªçc d·ªØ li·ªáu"]
            },
            {
                "nghe": "L√£nh ƒë·∫°o c·∫•p cao (C-suite, Legal Head, Business Director, Marketing Head)",
                "nhu_cau": "Doanh nghi·ªáp VN thi·∫øu qu·∫£n l√Ω c·∫•p cao, ƒë·∫∑c bi·ªát trong lƒ©nh v·ª±c ph√°p l√Ω, t√†i ch√≠nh, marketing",
                "luong": "Head of Legal ~‚Ç´330M/nƒÉm; C‚Äësuite ~‚Ç´230‚Äì490M/nƒÉm",
                "tang_truong": "Nhu c·∫ßu tƒÉng tr∆∞·ªüng 20%/nƒÉm cho c√°c v·ªã tr√≠ qu·∫£n l√Ω c·∫•p cao",
                "nganh_goi_y": ["Lu·∫≠t", "Qu·∫£n tr·ªã kinh doanh", "Marketing", "T√†i ch√≠nh ng√¢n h√†ng"]
            }
        ]
        
    def load_dtu_programs(self, file_path: str) -> pd.DataFrame:
        """
        Load d·ªØ li·ªáu ng√†nh h·ªçc ƒê·∫°i h·ªçc Duy T√¢n t·ª´ file Excel/CSV
        """
        try:
            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
                df = pd.read_excel(file_path)
            elif file_path.endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8')
            else:
                raise ValueError("File ph·∫£i l√† .xlsx, .xls ho·∫∑c .csv")
            
            print(f"‚úÖ ƒê√£ load {len(df)} ng√†nh h·ªçc t·ª´ file: {file_path}")
            return df
        
        except Exception as e:
            print(f"‚ùå L·ªói khi load file: {e}")
            return None

    def preprocess_text(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω text ƒë·ªÉ c·∫£i thi·ªán matching
        """
        if pd.isna(text):
            return ""
        
        # Convert to lowercase v√† remove special chars
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Normalize m·ªôt s·ªë t·ª´ kh√≥a
        replacements = {
            'cntt': 'c√¥ng ngh·ªá th√¥ng tin',
            'it': 'c√¥ng ngh·ªá th√¥ng tin',
            'ai': 'tr√≠ tu·ªá nh√¢n t·∫°o',
            'ml': 'machine learning',
            'iot': 'internet of things'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
            
        return text

    def calculate_similarity_transformer(self, job_suggestions: List[str], dtu_programs: List[str]) -> np.ndarray:
        """
        T√≠nh similarity s·ª≠ d·ª•ng Sentence Transformer
        """
        if self.model is None:
            return self.calculate_similarity_fallback(job_suggestions, dtu_programs)
        
        try:
            # Encode c√°c text th√†nh vectors
            job_embeddings = self.model.encode(job_suggestions)
            program_embeddings = self.model.encode(dtu_programs)
            
            # T√≠nh cosine similarity
            similarity_matrix = cosine_similarity(
                job_embeddings.reshape(len(job_suggestions), -1),
                program_embeddings.reshape(len(dtu_programs), -1)
            )
            
            return similarity_matrix
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi t√≠nh similarity v·ªõi transformer: {e}")
            return self.calculate_similarity_fallback(job_suggestions, dtu_programs)

    def calculate_similarity_fallback(self, job_suggestions: List[str], dtu_programs: List[str]) -> np.ndarray:
        """
        Fallback method: T√≠nh similarity d·ª±a tr√™n keyword matching
        """
        similarity_matrix = np.zeros((len(job_suggestions), len(dtu_programs)))
        
        for i, job in enumerate(job_suggestions):
            job_processed = self.preprocess_text(job)
            job_words = set(job_processed.split())
            
            for j, program in enumerate(dtu_programs):
                program_processed = self.preprocess_text(program)
                program_words = set(program_processed.split())
                
                # Jaccard similarity
                intersection = job_words.intersection(program_words)
                union = job_words.union(program_words)
                
                if len(union) > 0:
                    similarity_matrix[i][j] = len(intersection) / len(union)
        
        return similarity_matrix

    def map_jobs_to_programs(self, dtu_df: pd.DataFrame, threshold: float = 0.3) -> pd.DataFrame:
        """
        Map c√°c ng√†nh ngh·ªÅ hot v·ªõi ng√†nh h·ªçc DTU s·ª≠ d·ª•ng AI
        """
        print("üîÑ ƒêang th·ª±c hi·ªán mapping v·ªõi AI...")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu DTU programs
        dtu_programs = []
        for _, row in dtu_df.iterrows():
            program_text = f"{row['Ng√†nh']} {row['Chuy√™n ng√†nh']}"
            dtu_programs.append(self.preprocess_text(program_text))
        
        results = []
        
        for job_trend in self.job_trends:
            print(f"üìä Mapping: {job_trend['nghe'][:50]}...")
            
            # T√≠nh similarity gi·ªØa g·ª£i √Ω ng√†nh v√† DTU programs  
            job_suggestions_processed = [self.preprocess_text(suggestion) for suggestion in job_trend['nganh_goi_y']]
            similarity_matrix = self.calculate_similarity_transformer(job_suggestions_processed, dtu_programs)
            
            # T√¨m matches t·ªët nh·∫•t
            matches = []
            for i, suggestion in enumerate(job_trend['nganh_goi_y']):
                # L·∫•y top 3 matches cho m·ªói g·ª£i √Ω
                similarities = similarity_matrix[i]
                top_indices = np.argsort(similarities)[::-1][:3]
                
                for idx in top_indices:
                    if similarities[idx] >= threshold:
                        matches.append({
                            'suggestion': suggestion,
                            'dtu_program': dtu_df.iloc[idx],
                            'similarity': similarities[idx]
                        })
            
            # Lo·∫°i b·ªè duplicate v√† sort theo similarity
            unique_matches = {}
            for match in matches:
                key = (match['dtu_program']['M√£ CN'])
                if key not in unique_matches or match['similarity'] > unique_matches[key]['similarity']:
                    unique_matches[key] = match
            
            # Sort v√† l·∫•y top matches
            sorted_matches = sorted(unique_matches.values(), key=lambda x: x['similarity'], reverse=True)[:5]
            
            # T·∫°o output
            matched_programs = []
            for match in sorted_matches:
                prog = match['dtu_program']
                matched_programs.append({
                    'chuyen_nganh': prog['Chuy√™n ng√†nh'],
                    'ma_cn': prog['M√£ CN'],
                    'similarity_score': round(match['similarity'], 3),
                    'matched_suggestion': match['suggestion']
                })
            
            results.append({
                'nghe_nganh_hot': job_trend['nghe'],
                'nhu_cau_chuyen_bien': job_trend['nhu_cau'], 
                'muc_luong_uoc_tinh': job_trend['luong'],
                'tang_truong_tuyen_dung': job_trend['tang_truong'],
                'cac_nganh_dtu_phu_hop': matched_programs
            })
        
        return pd.DataFrame(results)

    def export_results(self, results_df: pd.DataFrame, output_path: str = "xu_huong_viec_lam_mapped.xlsx"):
        """
        Xu·∫•t k·∫øt qu·∫£ ra file Excel v·ªõi format ƒë·∫πp
        """
        print(f"üì§ ƒêang xu·∫•t k·∫øt qu·∫£ ra: {output_path}")
        
        # T·∫°o data cho export v·ªõi format m·ªõi
        export_data = []
        
        for _, row in results_df.iterrows():
            programs = row['cac_nganh_dtu_phu_hop']
            
            if programs:  # N·∫øu c√≥ match
                programs_text = []
                for prog in programs:
                    programs_text.append(f"‚Ä¢ {prog['chuyen_nganh']} (M√£: {prog['ma_cn']}) - ƒê·ªô ph√π h·ª£p: {prog['similarity_score']}")
                programs_str = "\n".join(programs_text)
            else:
                programs_str = "Ch∆∞a t√¨m th·∫•y ng√†nh ph√π h·ª£p"
            
            export_data.append({
                'Ngh·ªÅ/ng√†nh hot': row['nghe_nganh_hot'],
                'Nhu c·∫ßu / Chuy·ªÉn bi·∫øn x√£ h·ªôi': row['nhu_cau_chuyen_bien'],
                'M·ª©c l∆∞∆°ng ∆∞·ªõc t√≠nh': row['muc_luong_uoc_tinh'],
                'S·ªë li·ªáu tƒÉng tr∆∞·ªüng / Tuy·ªÉn d·ª•ng': row['tang_truong_tuyen_dung'],
                'C√°c ng√†nh DTU ph√π h·ª£p': programs_str
            })
        
        # Export to Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            export_df = pd.DataFrame(export_data)
            export_df.to_excel(writer, sheet_name='Xu_huong_viec_lam_DTU', index=False)
            
            # Adjust column widths
            worksheet = writer.sheets['Xu_huong_viec_lam_DTU']
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        print(f"‚úÖ ƒê√£ xu·∫•t th√†nh c√¥ng: {output_path}")
        return output_path

    def generate_summary_report(self, results_df: pd.DataFrame) -> Dict:
        """
        T·∫°o b√°o c√°o t√≥m t·∫Øt k·∫øt qu·∫£ mapping
        """
        total_trends = len(results_df)
        total_matches = sum(len(row['cac_nganh_dtu_phu_hop']) for _, row in results_df.iterrows())
        
        # Th·ªëng k√™ ng√†nh ƒë∆∞·ª£c match nhi·ªÅu nh·∫•t
        all_programs = []
        for _, row in results_df.iterrows():
            all_programs.extend([p['chuyen_nganh'] for p in row['cac_nganh_dtu_phu_hop']])
        
        from collections import Counter
        top_programs = Counter(all_programs).most_common(5)
        
        summary = {
            'total_job_trends': total_trends,
            'total_matches_found': total_matches,
            'average_matches_per_trend': round(total_matches / total_trends, 2),
            'top_matched_programs': top_programs,
            'coverage_rate': f"{round((total_matches / total_trends) * 100, 1)}% xu h∆∞·ªõng c√≥ match"
        }
        
        return summary

def main():
    """
    H√†m main ƒë·ªÉ ch·∫°y to√†n b·ªô process
    """
    print("üöÄ Kh·ªüi ƒë·ªông AI Tuy·ªÉn Sinh Job Trend Mapper")
    print("=" * 60)
    
    # Kh·ªüi t·∫°o mapper
    mapper = JobTrendMapper()
    
    # Load d·ªØ li·ªáu DTU (thay ƒë·ªïi path theo file th·ª±c t·∫ø)
    # C√≥ th·ªÉ d√πng d·ªØ li·ªáu b·∫°n cung c·∫•p ho·∫∑c file Excel
    print("\nüìÅ ƒêang t·∫°o d·ªØ li·ªáu m·∫´u DTU...")
    
    # Sample data t·ª´ d·ªØ li·ªáu b·∫°n cung c·∫•p
    sample_dtu_data = {
        'TT': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
        'M√£ Ng√†nh': ['7480103', '7480103', '7480202', '7480101', '7480101', '7480107', 
                     '7460108', '7460108', '7340101', '7340115', '7340201', '7340404'],
        'Ng√†nh': ['K·ªπ thu·∫≠t Ph·∫ßn m·ªÅm', 'K·ªπ thu·∫≠t Ph·∫ßn m·ªÅm', 'An to√†n Th√¥ng tin', 
                 'Khoa h·ªçc M√°y t√≠nh', 'Khoa h·ªçc M√°y t√≠nh', 'Tr√≠ tu·ªá Nh√¢n t·∫°o',
                 'Khoa h·ªçc D·ªØ li·ªáu', 'Khoa h·ªçc D·ªØ li·ªáu', 'Qu·∫£n tr·ªã Kinh doanh',
                 'Marketing', 'T√†i ch√≠nh - Ng√¢n h√†ng', 'Qu·∫£n tr·ªã Nh√¢n l·ª±c'],
        'Chuy√™n ng√†nh': ['C√¥ng ngh·ªá Ph·∫ßn m·ªÅm', 'Thi·∫øt k·∫ø Games v√† Multimedia', 'An to√†n Th√¥ng tin',
                        'Khoa h·ªçc M√°y t√≠nh', 'K·ªπ thu·∫≠t M√°y t√≠nh', 'Tr√≠ tu·ªá Nh√¢n t·∫°o',
                        'Khoa h·ªçc D·ªØ li·ªáu', 'Big Data & Machine Learning', 'Qu·∫£n tr·ªã Kinh doanh T·ªïng h·ª£p',
                        'Digital Marketing', 'T√†i ch√≠nh Doanh nghi·ªáp', 'Qu·∫£n tr·ªã Nh√¢n l·ª±c'],
        'M√£ CN': ['102', '122', '124', '130', '128', '121', '135', '115', '400', '402', '403', '417']
    }
    
    dtu_df = pd.DataFrame(sample_dtu_data)
    
    if dtu_df is not None:
        print(f"‚úÖ ƒê√£ load {len(dtu_df)} ng√†nh h·ªçc DTU")
        
        # Th·ª±c hi·ªán mapping
        print("\nü§ñ B·∫Øt ƒë·∫ßu AI mapping process...")
        results = mapper.map_jobs_to_programs(dtu_df, threshold=0.2)
        
        # Xu·∫•t k·∫øt qu·∫£
        print("\nüìä K·∫øt qu·∫£ mapping:")
        for i, (_, row) in enumerate(results.iterrows(), 1):
            print(f"\n{i}. {row['nghe_nganh_hot'][:60]}...")
            matches = row['cac_nganh_dtu_phu_hop']
            if matches:
                for match in matches[:3]:  # Show top 3
                    print(f"   ‚Üí {match['chuyen_nganh']} (M√£: {match['ma_cn']}) - Ph√π h·ª£p: {match['similarity_score']}")
            else:
                print("   ‚Üí Kh√¥ng t√¨m th·∫•y ng√†nh ph√π h·ª£p")
        
        # Export to Excel
        output_file = mapper.export_results(results)
        
        # T·∫°o b√°o c√°o t√≥m t·∫Øt
        summary = mapper.generate_summary_report(results)
        print(f"\nüìã B√ÅO C√ÅO T√ìM T·∫ÆT:")
        print(f"   ‚Ä¢ T·ªïng xu h∆∞·ªõng vi·ªác l√†m: {summary['total_job_trends']}")
        print(f"   ‚Ä¢ T·ªïng matches t√¨m ƒë∆∞·ª£c: {summary['total_matches_found']}")
        print(f"   ‚Ä¢ Trung b√¨nh matches/xu h∆∞·ªõng: {summary['average_matches_per_trend']}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá coverage: {summary['coverage_rate']}")
        
        print(f"\nüéâ Ho√†n th√†nh! File k·∫øt qu·∫£: {output_file}")
    
    else:
        print("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu DTU")

if __name__ == "__main__":
    # C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt
    print("üì¶ C·∫ßn c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau:")
    print("pip install sentence-transformers pandas openpyxl scikit-learn numpy")
    print("\n" + "="*60)
    
    main()