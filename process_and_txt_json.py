import re
import json
import sys
import subprocess

# Thá»­ import, náº¿u chÆ°a cÃ³ thÃ¬ cÃ i
try:
    from docx import Document
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
    from docx import Document


def read_docx(path: str) -> str:
    """Äá»c toÃ n bá»™ ná»™i dung tá»« file .docx thÃ nh chuá»—i text."""
    doc = Document(path)
    return "\n".join(p.text for p in doc.paragraphs if p.text.strip())


def format_ma_cn(ma_cn: str) -> str:
    """Format mÃ£ CN thÃ nh dáº¡ng compact: 116 (CMU) -> 116CMU"""
    if not ma_cn:
        return ""
    
    # Remove spaces and parentheses, keep only letters and numbers
    formatted = re.sub(r'[\s\(\)]', '', ma_cn.strip())
    return formatted


def parse_structured_data(raw_text: str) -> dict:
    """Parse dá»¯ liá»‡u thÃ nh cáº¥u trÃºc ngÃ nh -> chuyÃªn ngÃ nh."""
    
    # TÃ¬m táº¥t cáº£ cÃ¡c pattern "NgÃ nh ... MÃ£ ngÃ nh: XXX"
    nganh_pattern = r'NgÃ nh\s+(.+?)\s*-\s*MÃ£ ngÃ nh:\s*(\d+)'
    
    nganh_matches = list(re.finditer(nganh_pattern, raw_text, re.IGNORECASE))
    
    if not nganh_matches:
        print("KhÃ´ng tÃ¬m tháº¥y ngÃ nh nÃ o!")
        return {"nganh": []}
    
    result = {"nganh": []}
    
    # Xá»­ lÃ½ tá»«ng ngÃ nh
    for i, nganh_match in enumerate(nganh_matches):
        ten_nganh = nganh_match.group(1).strip()
        ma_nganh = nganh_match.group(2).strip()
        
        # TÃ¬m vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a ngÃ nh nÃ y
        start_pos = nganh_match.start()
        
        if i + 1 < len(nganh_matches):
            end_pos = nganh_matches[i + 1].start()
        else:
            end_pos = len(raw_text)
        
        # Láº¥y text cá»§a ngÃ nh nÃ y
        nganh_text = raw_text[start_pos:end_pos]
        
        # TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text nÃ y
        chuyen_nganh_list = find_chuyen_nganh_in_text(nganh_text, ten_nganh, ma_nganh)
        
        # Má»—i chuyÃªn ngÃ nh táº¡o thÃ nh má»™t object riÃªng biá»‡t vá»›i keys chuáº©n
        for chuyen_nganh_data in chuyen_nganh_list:
            nganh_obj = {
                "NgÃ nh": ten_nganh,
                "MÃ£ NgÃ nh": ma_nganh,
                "ChuyÃªn NgÃ nh": chuyen_nganh_data["TÃªn"],
                "MÃ£ CN": chuyen_nganh_data["MÃ£ CN"],
                **{k: v for k, v in chuyen_nganh_data.items() if k not in ["TÃªn", "MÃ£ CN"]}
            }
            result["nganh"].append(nganh_obj)
    
    return result


def find_chuyen_nganh_in_text(nganh_text: str, ten_nganh: str, ma_nganh: str) -> list:
    """TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text cá»§a má»™t ngÃ nh."""
    
    # Patterns Ä‘á»ƒ tÃ¬m chuyÃªn ngÃ nh - Cáº£i thiá»‡n Ä‘á»ƒ trÃ¡nh láº¥y thÃªm text
    chuyen_nganh_patterns = [
        r'(?:TÃªn\s+)?ChuyÃªn ngÃ nh[:\s]+(.+?)\s*-\s*MÃ£ chuyÃªn ngÃ nh[:\s]*([A-Za-z0-9\s\(\)]+?)(?=\s*[:\n]|$)',
        r'ChuyÃªn ngÃ nh[:\s]+(.+?)\s*-\s*MÃ£[:\s]*([A-Za-z0-9\s\(\)]+?)(?=\s*[:\n]|$)'
    ]
    
    chuyen_nganh_matches = []
    
    # TÃ¬m táº¥t cáº£ chuyÃªn ngÃ nh trong text
    for pattern in chuyen_nganh_patterns:
        matches = list(re.finditer(pattern, nganh_text, re.IGNORECASE))
        if matches:
            chuyen_nganh_matches = matches
            break
    
    if not chuyen_nganh_matches:
        print(f"KhÃ´ng tÃ¬m tháº¥y chuyÃªn ngÃ nh nÃ o trong ngÃ nh {ten_nganh}")
        return []
    
    result = []
    
    # Xá»­ lÃ½ tá»«ng chuyÃªn ngÃ nh
    for i, match in enumerate(chuyen_nganh_matches):
        ten_chuyen_nganh = match.group(1).strip()
        ma_chuyen_nganh = match.group(2).strip()
        
        # Format mÃ£ CN thÃ nh dáº¡ng compact
        ma_chuyen_nganh_formatted = format_ma_cn(ma_chuyen_nganh)
        
        # TÃ¬m ná»™i dung cá»§a chuyÃªn ngÃ nh nÃ y
        content_start = match.end()
        
        # TÃ¬m Ä‘iá»ƒm káº¿t thÃºc (trÆ°á»›c chuyÃªn ngÃ nh tiáº¿p theo hoáº·c cuá»‘i text)
        if i + 1 < len(chuyen_nganh_matches):
            content_end = chuyen_nganh_matches[i + 1].start()
        else:
            # TÃ¬m Ä‘iá»ƒm káº¿t thÃºc khÃ¡c (ngÃ nh má»›i, hoáº·c cuá»‘i text)
            next_nganh = re.search(r'NgÃ nh\s+.+?\s*-\s*MÃ£ ngÃ nh:', nganh_text[content_start:], re.IGNORECASE)
            if next_nganh:
                content_end = content_start + next_nganh.start()
            else:
                content_end = len(nganh_text)
        
        # Láº¥y ná»™i dung
        content_text = nganh_text[content_start:content_end].strip()
        
        # Parse cÃ¡c field content
        content_fields = parse_content_fields(content_text)
        
        # Táº¡o object chuyÃªn ngÃ nh vá»›i keys cÃ³ dáº¥u viáº¿t hoa
        chuyen_nganh_obj = {
            "TÃªn": ten_chuyen_nganh,
            "MÃ£ CN": ma_chuyen_nganh_formatted,
            **content_fields
        }
        
        # XÃ³a cÃ¡c trÆ°á»ng khÃ´ng cáº§n thiáº¿t
        fields_to_remove = ["Tuyá»ƒn Sinh", "LiÃªn Há»‡"]
        for field in fields_to_remove:
            if field in chuyen_nganh_obj:
                del chuyen_nganh_obj[field]
        
        result.append(chuyen_nganh_obj)
        print(f"  âœ… TÃ¬m tháº¥y chuyÃªn ngÃ nh: {ten_chuyen_nganh} ({ma_chuyen_nganh} -> {ma_chuyen_nganh_formatted})")
    
    return result


def parse_content_fields(content_text: str) -> dict:
    """Parse cÃ¡c field: Giá»›i thiá»‡u, Má»¥c tiÃªu, ChÆ°Æ¡ng trÃ¬nh, etc."""
    
    keyword_map = {
        "Giá»›i thiá»‡u:": "Giá»›i Thiá»‡u",
        "Má»¥c tiÃªu:": "Má»¥c TiÃªu",
        "ChÆ°Æ¡ng trÃ¬nh:": "ChÆ°Æ¡ng TrÃ¬nh", 
        "CÆ¡ há»™i:": "CÆ¡ Há»™i",
        "LiÃªn há»‡:": "LiÃªn Há»‡",
        "Tuyá»ƒn sinh:": "Tuyá»ƒn Sinh"
    }
    
    result = {}
    
    # Normalize content first
    content_text = re.sub(r'\n+', ' ', content_text)  # Replace multiple newlines with space
    content_text = re.sub(r'\s+', ' ', content_text)  # Normalize whitespace
    
    # Táº¡o pattern Ä‘á»ƒ tÃ¬m táº¥t cáº£ sections
    all_keywords = list(keyword_map.keys())
    
    # TÃ¬m positions cá»§a táº¥t cáº£ keywords
    keyword_positions = []
    for keyword in all_keywords:
        pattern = rf'\b{re.escape(keyword)}[:\s]'
        for match in re.finditer(pattern, content_text, re.IGNORECASE):
            keyword_positions.append((match.start(), match.end(), keyword))
    
    # Sort theo position
    keyword_positions.sort()
    
    # Extract content cho tá»«ng keyword
    for i, (start, end, keyword) in enumerate(keyword_positions):
        # TÃ¬m Ä‘iá»ƒm káº¿t thÃºc cho keyword nÃ y
        if i + 1 < len(keyword_positions):
            next_start = keyword_positions[i + 1][0]
            section_text = content_text[end:next_start].strip()
        else:
            section_text = content_text[end:].strip()
        
        # Clean section text
        section_text = clean_section_content(section_text, keyword)
        
        if section_text and keyword in keyword_map:
            result[keyword_map[keyword]] = section_text
    
    return result


def clean_section_content(text: str, keyword: str) -> str:
    """Clean vÃ  format ná»™i dung cá»§a tá»«ng section."""
    if not text:
        return ""
    
    # Remove common prefixes
    prefixes_to_remove = [
        "Ä‘Ã o táº¡o:",
        "nghá» nghiá»‡p:",
    ]
    
    text_lower = text.lower()
    for prefix in prefixes_to_remove:
        if text_lower.startswith(prefix):
            text = text[len(prefix):].strip()
            break
    
    # Remove bullet points
    text = re.sub(r'^\s*â€¢\s*', '', text, flags=re.MULTILINE)
    
    # Handle special cases for "LiÃªn há»‡"
    if keyword == "LiÃªn há»‡:":
        return format_contact_info(text)
    
    # Truncate if text is too long and seems to contain next section
    if len(text) > 1000:
        # Look for signs of next section
        for next_keyword in ["NgÃ nh", "ChuyÃªn ngÃ nh", "Website:", "Email:", "Äiá»‡n thoáº¡i:"]:
            pos = text.find(next_keyword)
            if pos > 100:  # Only truncate if found after reasonable content
                text = text[:pos].strip()
                break
    
    return text.strip()


def format_contact_info(text: str) -> str:
    """Format láº¡i thÃ´ng tin liÃªn há»‡."""
    # Extract contact details
    website_match = re.search(r'Website:\s*([^\s\n]+)', text, re.IGNORECASE)
    email_match = re.search(r'Email:\s*([^\s\n]+)', text, re.IGNORECASE)
    phone_match = re.search(r'Äiá»‡n thoáº¡i:\s*([^\n\r]+?)(?=\s*(?:NgÃ nh|ChuyÃªn ngÃ nh|$))', text, re.IGNORECASE)
    
    if not (website_match or email_match or phone_match):
        return text.strip()
    
    formatted_parts = []
    if website_match:
        formatted_parts.append(f"Website: {website_match.group(1)}")
    if email_match:
        formatted_parts.append(f"Email: {email_match.group(1)}")
    if phone_match:
        phone_text = phone_match.group(1).strip()
        phone_text = re.sub(r'\s*â€“\s*', ' - ', phone_text)  # Normalize dashes
        formatted_parts.append(f"Äiá»‡n thoáº¡i: {phone_text}")
    
    return " | ".join(formatted_parts)


if __name__ == "__main__":
    files = [
        "TRÆ¯á»œNG CÃ”NG NGHá»†.docx",
        "KHOA ÄTQT.docx",
        "TRÆ¯á»œNG DU Lá»ŠCH.docx", 
        "TRÆ¯á»œNG KHMT.docx",
        "TRÆ¯á»œNG KINH Táº¾&KINH DOANH.docx",
        "TRÆ¯á»œNG NN&XHNV.docx",
        "TRUNG TÃ‚M KT&CN VIá»†T-NHáº¬T.docx",
        "KHOA Y-DÆ¯á»¢C.docx"
    ]

    all_data = {}

    for file in files:
        try:
            print(f"\nğŸ” Processing {file}...")
            text = read_docx(file)
            if text.strip():
                result = parse_structured_data(text)
                file_key = file.replace(".docx", "")
                all_data[file_key] = result
                
                # Thá»‘ng kÃª Ä‘Ãºng - má»—i item trong nganh array lÃ  1 chuyÃªn ngÃ nh
                total_chuyen_nganh = len(result.get('nganh', []))
                print(f"âœ… {file}: {total_chuyen_nganh} chuyÃªn ngÃ nh")
            else:
                print(f"âš ï¸  {file}: No content found")
        except Exception as e:
            print(f"âŒ Error processing {file}: {e}")

    # LÆ°u file JSON
    if all_data:
        with open("mo_ta_nganh.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u vÃ o mo_ta_nganh_final.json")
        
        # Thá»‘ng kÃª tá»•ng Ä‘Ãºng
        total_chuyen_nganh = sum(len(school.get('nganh', [])) for school in all_data.values())
        print(f"ğŸ“Š Tá»•ng cá»™ng: {len(all_data)} trÆ°á»ng, {total_chuyen_nganh} chuyÃªn ngÃ nh")
    else:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")